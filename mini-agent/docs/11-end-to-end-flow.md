# ç«¯åˆ°ç«¯æ•°æ®æµä¸æ‰§è¡Œé“¾è·¯

## æ–‡æ¡£ç›®æ ‡
æœ¬æ–‡ä»¶èšç„¦â€œä»ç”¨æˆ·è¾“å…¥ query åˆ°æœ€ç»ˆè¾“å‡ºâ€çš„å®Œæ•´é“¾è·¯ï¼Œç»™å‡ºæ•°æ®æµèµ°å‘ã€æ ¸å¿ƒæ•°æ®ç»“æ„ä¸å…³é”®è®¾è®¡åŠ¨æœºï¼Œä¾¿äºå¿«é€Ÿç†è§£æ•´ä½“è¿è¡Œæœºåˆ¶ã€‚

## å®è§‚æ•°æ®æµï¼ˆå…¨æ™¯ï¼‰
```mermaid
sequenceDiagram
    User->>CLI: input query
    CLI->>Agent: add_user_message()
    Agent->>Agent: summarize if needed
    Agent->>LLMClient: generate(messages, tools)
    LLMClient-->>Agent: LLMResponse
    alt tool_calls present
        Agent->>Tool: execute(arguments)
        Tool-->>Agent: ToolResult
        Agent->>Agent: append tool message
        Agent->>LLMClient: generate(messages, tools)
    else no tool_calls
        Agent-->>CLI: final content
    end
    CLI-->>User: render output
```

## æ ¸å¿ƒæ•°æ®ç»“æ„ä¸è§’è‰²
### Message / ToolCall / LLMResponse
æ¶ˆæ¯ä¸å·¥å…·è°ƒç”¨æ˜¯ LLM äº¤äº’çš„ç»Ÿä¸€è½½ä½“ï¼ŒåŒ…å« roleã€contentã€thinking ä¸ tool_calls ç­‰å­—æ®µã€‚

```python
# File: mini_agent/schema/schema.py | Lines: 14-55 | Description: Message and response models
class FunctionCall(BaseModel):
    """Function call details."""

    name: str
    arguments: dict[str, Any]  # Function arguments as dict


class ToolCall(BaseModel):
    """Tool call structure."""

    id: str
    type: str  # "function"
    function: FunctionCall


class Message(BaseModel):
    """Chat message."""

    role: str  # "system", "user", "assistant", "tool"
    content: str | list[dict[str, Any]]  # Can be string or list of content blocks
    thinking: str | None = None  # Extended thinking content for assistant messages
    tool_calls: list[ToolCall] | None = None
    tool_call_id: str | None = None
    name: str | None = None  # For tool role


class LLMResponse(BaseModel):
    """LLM response."""

    content: str
    thinking: str | None = None  # Extended thinking blocks
    tool_calls: list[ToolCall] | None = None
    finish_reason: str
    usage: TokenUsage | None = None  # Token usage from API response
```

### Tool / ToolResult
å·¥å…·è°ƒç”¨è¿”å›ç»Ÿä¸€çš„ç»“æ„ï¼Œä¾¿äºæ—¥å¿—ä¸æ¶ˆæ¯å›å†™ã€‚

```python
# File: mini_agent/tools/base.py | Lines: 8-44 | Description: Tool and ToolResult
class ToolResult(BaseModel):
    """Tool execution result."""

    success: bool
    content: str = ""
    error: str | None = None


class Tool:
    """Base class for all tools."""

    @property
    def name(self) -> str:
        """Tool name."""
        raise NotImplementedError

    @property
    def description(self) -> str:
        """Tool description."""
        raise NotImplementedError

    @property
    def parameters(self) -> dict[str, Any]:
        """Tool parameters schema (JSON Schema format)."""
        raise NotImplementedError

    async def execute(self, *args, **kwargs) -> ToolResult:  # type: ignore
        """Execute the tool with arbitrary arguments."""
        raise NotImplementedError

    def to_schema(self) -> dict[str, Any]:
        """Convert tool to Anthropic tool schema."""
        return {
            "name": self.name,
            "description": self.description,
            "input_schema": self.parameters,
        }
```

## ç«¯åˆ°ç«¯æ‰§è¡Œæ­¥éª¤ï¼ˆä»è¾“å…¥åˆ°è¾“å‡ºï¼‰

### æ­¥éª¤1ï¼šCLI è·å–ç”¨æˆ·è¾“å…¥
**æ•°æ®æµ**: terminal â†’ user_input

```python
# File: mini_agent/cli.py | Lines: 497-509 | Description: Read user input
# 10. Interactive loop
while True:
    try:
        # Get user input using prompt_toolkit
        # Use styled list for robust coloring
        user_input = await session.prompt_async(
            [
                ("class:prompt", "You"),
                ("", " â€º "),
            ],
            multiline=False,
            enable_history_search=True,
        )
        user_input = user_input.strip()
```

### æ­¥éª¤2ï¼šå†™å…¥æ¶ˆæ¯å¹¶è§¦å‘ Agent
**æ•°æ®æµ**: user_input â†’ messages[] â†’ Agent.run()

```python
# File: mini_agent/cli.py | Lines: 554-557 | Description: Delegate to Agent
# Run Agent
print(f"\n{Colors.BRIGHT_BLUE}Agent{Colors.RESET} {Colors.DIM}â€º{Colors.RESET} {Colors.DIM}Thinking...{Colors.RESET}\n")
agent.add_user_message(user_input)
_ = await agent.run()
```

```python
# File: mini_agent/agent.py | Lines: 81-83 | Description: Add user message
def add_user_message(self, content: str):
    """Add a user message to history."""
    self.messages.append(Message(role="user", content=content))
```

### æ­¥éª¤3ï¼šAgent åˆå§‹åŒ–ä¸å·¥ä½œåŒºæ³¨å…¥
**æ•°æ®æµ**: config â†’ system_prompt + workspace info â†’ messages[]

```python
# File: mini_agent/agent.py | Lines: 45-71 | Description: Workspace injection
def __init__(
    self,
    llm_client: LLMClient,
    system_prompt: str,
    tools: list[Tool],
    max_steps: int = 50,
    workspace_dir: str = "./workspace",
    token_limit: int = 80000,  # Summary triggered when tokens exceed this value
):
    self.llm = llm_client
    self.tools = {tool.name: tool for tool in tools}
    self.max_steps = max_steps
    self.token_limit = token_limit
    self.workspace_dir = Path(workspace_dir)

    # Ensure workspace exists
    self.workspace_dir.mkdir(parents=True, exist_ok=True)

    # Inject workspace information into system prompt if not already present
    if "Current Workspace" not in system_prompt:
        workspace_info = f"\n\n## Current Workspace\nYou are currently working in: `{self.workspace_dir.absolute()}`\nAll relative paths will be resolved relative to this directory."
        system_prompt = system_prompt + workspace_info

    self.system_prompt = system_prompt

    # Initialize message history
    self.messages: list[Message] = [Message(role="system", content=system_prompt)]
```

### æ­¥éª¤4ï¼šæ‰§è¡Œå¾ªç¯ä¸æ‘˜è¦åˆ¤å®š
**æ•°æ®æµ**: messages[] â†’ summarize check â†’ messages[]ï¼ˆå¯èƒ½è¢«å‹ç¼©ï¼‰

```python
# File: mini_agent/agent.py | Lines: 281-291 | Description: Run loop start and summarization
async def run(self) -> str:
    """Execute agent loop until task is complete or max steps reached."""
    # Start new run, initialize log file
    self.logger.start_new_run()
    print(f"{Colors.DIM}ğŸ“ Log file: {self.logger.get_log_file_path()}{Colors.RESET}")

    step = 0

    while step < self.max_steps:
        # Check and summarize message history to prevent context overflow
        await self._summarize_messages()
```

```python
# File: mini_agent/agent.py | Lines: 142-154 | Description: Summarization trigger rules
async def _summarize_messages(self):
    """Message history summarization: summarize conversations between user messages when tokens exceed limit

    Strategy (Agent mode):
    - Keep all user messages (these are user intents)
    - Summarize content between each user-user pair (agent execution process)
    - If last round is still executing (has agent/tool messages but no next user), also summarize
    - Structure: system -> user1 -> summary1 -> user2 -> summary2 -> user3 -> summary3 (if executing)

    Summary is triggered when EITHER:
    - Local token estimation exceeds limit
    - API reported total_tokens exceeds limit
    """
```

### æ­¥éª¤5ï¼šLLM è°ƒç”¨ä¸å“åº”å†™å›
**æ•°æ®æµ**: messages + tools â†’ LLMResponse â†’ messages[]

```python
# File: mini_agent/agent.py | Lines: 303-310 | Description: LLM request
# Get tool list for LLM call
tool_list = list(self.tools.values())

# Log LLM request and call LLM with Tool objects directly
self.logger.log_request(messages=self.messages, tools=tool_list)

try:
    response = await self.llm.generate(messages=self.messages, tools=tool_list)
```

```python
# File: mini_agent/agent.py | Lines: 335-356 | Description: Append response and finish check
# Add assistant message
assistant_msg = Message(
    role="assistant",
    content=response.content,
    thinking=response.thinking,
    tool_calls=response.tool_calls,
)
self.messages.append(assistant_msg)

# Print assistant response
if response.content:
    print(f"\n{Colors.BOLD}{Colors.BRIGHT_BLUE}ğŸ¤– Assistant:{Colors.RESET}")
    print(f"{response.content}")

# Check if task is complete (no tool calls)
if not response.tool_calls:
    return response.content
```

### æ­¥éª¤6ï¼šå·¥å…·è°ƒç”¨ä¸ç»“æœå›å†™
**æ•°æ®æµ**: ToolResult â†’ messages[]ï¼ˆtool appendedï¼‰â†’ ä¸‹ä¸€è½® LLM è°ƒç”¨

```python
# File: mini_agent/agent.py | Lines: 404-429 | Description: Tool result append
# Log tool execution result
self.logger.log_tool_result(
    tool_name=function_name,
    arguments=arguments,
    result_success=result.success,
    result_content=result.content if result.success else None,
    result_error=result.error if not result.success else None,
)

# Print result
if result.success:
    result_text = result.content
    if len(result_text) > 300:
        result_text = result_text[:300] + f"{Colors.DIM}...{Colors.RESET}"
    print(f"{Colors.BRIGHT_GREEN}âœ“ Result:{Colors.RESET} {result_text}")
else:
    print(f"{Colors.BRIGHT_RED}âœ— Error:{Colors.RESET} {Colors.RED}{result.error}{Colors.RESET}")

# Add tool result message
tool_msg = Message(
    role="tool",
    content=result.content if result.success else f"Error: {result.error}",
    tool_call_id=tool_call_id,
    name=function_name,
)
self.messages.append(tool_msg)
```

### æ­¥éª¤7ï¼šç»ˆæ­¢æ¡ä»¶
**æ•°æ®æµ**: response.content â†’ CLI outputï¼›æˆ– max_steps è§¦å‘ç»ˆæ­¢

```python
# File: mini_agent/agent.py | Lines: 433-435 | Description: Max steps reached
# Max steps reached
error_msg = f"Task couldn't be completed after {self.max_steps} steps."
print(f"\n{Colors.BRIGHT_YELLOW}âš ï¸  {error_msg}{Colors.RESET}")
```

## å…³é”®è®¾è®¡é€‰æ‹©ä¸æ”¶ç›Š
- **æ¶ˆæ¯åˆ—è¡¨ä½œä¸ºå”¯ä¸€ä¸Šä¸‹æ–‡è½½ä½“**ï¼š`messages` ç»Ÿä¸€æ‰¿è½½ç”¨æˆ·è¾“å…¥ã€LLM è¾“å‡ºã€å·¥å…·ç»“æœï¼Œç¡®ä¿æ¯è½®è°ƒç”¨çš„ä¸Šä¸‹æ–‡å¯è¿½æº¯ã€‚
- **æ‘˜è¦ä¸ token åŒé‡è§¦å‘**ï¼šæ—¢ä½¿ç”¨æœ¬åœ°ä¼°ç®—ï¼Œä¹Ÿä½¿ç”¨ API ç»Ÿè®¡ï¼Œå‡å°‘ä¸Šä¸‹æ–‡æº¢å‡ºçš„å¶å‘æ€§ã€‚
- **å·¥å…·ç»“æœå›å†™**ï¼šToolResult ä½œä¸º `role="tool"` æ¶ˆæ¯å†™å›ï¼Œä¿è¯ä¸‹ä¸€è½® LLM èƒ½â€œçœ‹åˆ°â€å·¥å…·è¾“å‡ºã€‚
- **workspace æ³¨å…¥**ï¼šç³»ç»Ÿæç¤ºè¯æ˜¾å¼å†™å…¥å·¥ä½œç›®å½•ï¼Œé™ä½ LLM è·¯å¾„æ­§ä¹‰ã€‚
- **ç»“æ„åŒ–æ—¥å¿—**ï¼šè¯·æ±‚/å“åº”/å·¥å…·ç»“æœç»Ÿä¸€è½ç›˜ï¼Œä¾¿äºå¤ç›˜ä¸è°ƒè¯•ï¼ˆè¯¦è§ `docs/03-architecture.md` ä¸ `docs/07-development-guide.md`ï¼‰ã€‚

## å…¨é“¾è·¯æ•°æ®æµæ‘˜è¦è¡¨
| é˜¶æ®µ | è¾“å…¥ | è¾“å‡º | å…³é”®ç»“æ„ |
| --- | --- | --- | --- |
| CLI è¾“å…¥ | terminal text | user_input | str |
| å…¥é˜Ÿæ¶ˆæ¯ | user_input | messages[] | Message(role="user") |
| LLM è°ƒç”¨ | messages + tools | LLMResponse | Message/ToolCall |
| å·¥å…·æ‰§è¡Œ | tool_calls | ToolResult | ToolResult |
| ç»“æœå›å†™ | ToolResult | messages[] | Message(role="tool") |
| ç»ˆæ­¢è¾“å‡º | response.content | CLI print | str |
