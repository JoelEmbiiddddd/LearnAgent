# æ ¸å¿ƒå·¥ä½œæœºåˆ¶

## ç«¯åˆ°ç«¯æ‰§è¡Œé“¾è·¯ï¼ˆä» Query åˆ°ç»“æœï¼‰

### æ¦‚è¿°
æ•´ä½“é“¾è·¯ç”± CLI è¾“å…¥é©±åŠ¨ï¼ŒAgent è´Ÿè´£ç»´æŠ¤æ¶ˆæ¯åˆ—è¡¨ã€è§¦å‘æ‘˜è¦ã€è°ƒç”¨ LLMã€åˆ†å‘å·¥å…·å¹¶å›å†™å·¥å…·ç»“æœï¼Œç›´åˆ°æ— å·¥å…·è°ƒç”¨ä¸ºæ­¢ã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    User->>CLI: input query
    CLI->>Agent: add_user_message()
    Agent->>Agent: summarize if needed
    Agent->>LLMClient: generate(messages, tools)
    LLMClient-->>Agent: LLMResponse
    alt tool_calls present
        Agent->>Tool: execute(arguments)
        Tool-->>Agent: ToolResult
        Agent->>Agent: append tool message
        Agent->>LLMClient: generate(messages, tools)
    else no tool_calls
        Agent-->>CLI: final content
    end
    CLI-->>User: render output
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: è·å–ç”¨æˆ·è¾“å…¥
**è§¦å‘æ¡ä»¶**: äº¤äº’å¼ CLI è¯»å–è¾“å…¥

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 497-509 | Description: Read user input
# 10. Interactive loop
while True:
    try:
        # Get user input using prompt_toolkit
        # Use styled list for robust coloring
        user_input = await session.prompt_async(
            [
                ("class:prompt", "You"),
                ("", " â€º "),
            ],
            multiline=False,
            enable_history_search=True,
        )
        user_input = user_input.strip()
```

**æ•°æ®æµ**: terminal input â†’ user_input
**è®¾è®¡åŠ¨æœº**: ä½¿ç”¨ prompt_toolkit å¹¶å¼€å¯ history searchï¼Œæå‡äº¤äº’å¯ç”¨æ€§ä¸è¾“å…¥æ•ˆç‡ã€‚

#### æ­¥éª¤2: äº¤ç»™ Agent æ‰§è¡Œ
**è§¦å‘æ¡ä»¶**: éå‘½ä»¤è¾“å…¥ï¼Œè¿›å…¥æ­£å¸¸å¯¹è¯

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 554-557 | Description: Delegate to Agent
# Run Agent
print(f"\n{Colors.BRIGHT_BLUE}Agent{Colors.RESET} {Colors.DIM}â€º{Colors.RESET} {Colors.DIM}Thinking...{Colors.RESET}\n")
agent.add_user_message(user_input)
_ = await agent.run()
```

**æ•°æ®æµ**: user_input â†’ messages[] â†’ Agent.run()
**è®¾è®¡åŠ¨æœº**: CLI ä»…è´Ÿè´£è¾“å…¥ä¸å§”æ‰˜ï¼ŒAgent ç»Ÿä¸€è´Ÿè´£æ‰§è¡Œå¾ªç¯ï¼Œé™ä½äº¤äº’å±‚ä¸æ ¸å¿ƒé€»è¾‘çš„è€¦åˆã€‚

#### æ­¥éª¤3: æ‰§è¡Œå¾ªç¯ä¸æ‘˜è¦åˆ¤å®š
**è§¦å‘æ¡ä»¶**: Agent.run() æ¯ä¸€æ­¥æ‰§è¡Œå‰

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 281-291 | Description: Run loop start and summarization
async def run(self) -> str:
    """Execute agent loop until task is complete or max steps reached."""
    # Start new run, initialize log file
    self.logger.start_new_run()
    print(f"{Colors.DIM}ğŸ“ Log file: {self.logger.get_log_file_path()}{Colors.RESET}")

    step = 0

    while step < self.max_steps:
        # Check and summarize message history to prevent context overflow
        await self._summarize_messages()
```

**æ•°æ®æµ**: messages[] â†’ summarize check â†’ messages[] (å¯èƒ½è¢«å‹ç¼©)
**è®¾è®¡åŠ¨æœº**: åœ¨æ¯æ­¥å¼€å§‹æ‰§è¡Œæ‘˜è¦åˆ¤å®šï¼Œä¼˜å…ˆä¿è¯ä¸Šä¸‹æ–‡ä¸æº¢å‡ºï¼ŒåŒæ—¶ä¿ç•™ç”¨æˆ·æ„å›¾ã€‚

#### æ­¥éª¤4: LLM è°ƒç”¨ä¸å“åº”å†™å›
**è§¦å‘æ¡ä»¶**: è¿›å…¥æ¯ä¸€è½®æ‰§è¡Œ

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 303-310 | Description: LLM request
# Get tool list for LLM call
tool_list = list(self.tools.values())

# Log LLM request and call LLM with Tool objects directly
self.logger.log_request(messages=self.messages, tools=tool_list)

try:
    response = await self.llm.generate(messages=self.messages, tools=tool_list)
```

```python
# File: mini_agent/agent.py | Lines: 335-356 | Description: Append response and finish check
# Add assistant message
assistant_msg = Message(
    role="assistant",
    content=response.content,
    thinking=response.thinking,
    tool_calls=response.tool_calls,
)
self.messages.append(assistant_msg)

# Print assistant response
if response.content:
    print(f"\n{Colors.BOLD}{Colors.BRIGHT_BLUE}ğŸ¤– Assistant:{Colors.RESET}")
    print(f"{response.content}")

# Check if task is complete (no tool calls)
if not response.tool_calls:
    return response.content
```

**æ•°æ®æµ**: messages + tools â†’ LLMResponse â†’ messages[]ï¼ˆassistant appendedï¼‰
**è®¾è®¡åŠ¨æœº**: å…ˆè®°å½•è¯·æ±‚å†è°ƒç”¨ LLMï¼Œå½¢æˆå¯è¿½æº¯è¯·æ±‚é“¾è·¯ï¼›å“åº”å†™å› messages ä½œä¸ºä¸‹ä¸€è½®æ‰§è¡Œä¸Šä¸‹æ–‡ã€‚

#### æ­¥éª¤5: å·¥å…·è°ƒç”¨ä¸ç»“æœå›å†™
**è§¦å‘æ¡ä»¶**: response.tool_calls éç©º

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 359-378 | Description: Tool call extraction
# Execute tool calls
for tool_call in response.tool_calls:
    tool_call_id = tool_call.id
    function_name = tool_call.function.name
    arguments = tool_call.function.arguments

    # Tool call header
    print(f"\n{Colors.BRIGHT_YELLOW}ğŸ”§ Tool Call:{Colors.RESET} {Colors.BOLD}{Colors.CYAN}{function_name}{Colors.RESET}")

    # Arguments (formatted display)
    print(f"{Colors.DIM}   Arguments:{Colors.RESET}")
    # Truncate each argument value to avoid overly long output
    truncated_args = {}
    for key, value in arguments.items():
        value_str = str(value)
        if len(value_str) > 200:
            truncated_args[key] = value_str[:200] + "..."
        else:
            truncated_args[key] = value
```

```python
# File: mini_agent/agent.py | Lines: 404-429 | Description: Tool result append
# Log tool execution result
self.logger.log_tool_result(
    tool_name=function_name,
    arguments=arguments,
    result_success=result.success,
    result_content=result.content if result.success else None,
    result_error=result.error if not result.success else None,
)

# Print result
if result.success:
    result_text = result.content
    if len(result_text) > 300:
        result_text = result_text[:300] + f"{Colors.DIM}...{Colors.RESET}"
    print(f"{Colors.BRIGHT_GREEN}âœ“ Result:{Colors.RESET} {result_text}")
else:
    print(f"{Colors.BRIGHT_RED}âœ— Error:{Colors.RESET} {Colors.RED}{result.error}{Colors.RESET}")

# Add tool result message
tool_msg = Message(
    role="tool",
    content=result.content if result.success else f"Error: {result.error}",
    tool_call_id=tool_call_id,
    name=function_name,
)
self.messages.append(tool_msg)
```

**æ•°æ®æµ**: ToolResult â†’ messages[]ï¼ˆtool appendedï¼‰â†’ ä¸‹ä¸€è½® LLM è°ƒç”¨
**è®¾è®¡åŠ¨æœº**: å°†å·¥å…·è¾“å‡ºä»¥ `role=\"tool\"` å†™å›æ¶ˆæ¯åˆ—è¡¨ï¼Œç¡®ä¿ LLM åœ¨ä¸‹ä¸€è½®å¯ä½¿ç”¨å·¥å…·ç»“æœã€‚

## æ ¸å¿ƒæµç¨‹ #1: CLI å¯åŠ¨ä¸è¿è¡Œæ—¶ç»„è£…

### æ¦‚è¿°
CLI è´Ÿè´£è§£æå‚æ•°ã€ç¡®å®šå·¥ä½œç›®å½•ã€åŠ è½½é…ç½®ã€åˆå§‹åŒ– LLM ä¸å·¥å…·ã€æ³¨å…¥ç³»ç»Ÿæç¤ºè¯å¹¶åˆ›å»º Agentï¼Œéšåè¿›å…¥äº¤äº’å¾ªç¯ã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    User->>CLI: start command
    CLI->>Config: load config
    CLI->>LLMClient: init provider
    CLI->>ToolInit: load tools
    CLI->>Agent: create instance
    CLI->>Agent: run interactive loop
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: è§£æ CLI å‚æ•°
**è§¦å‘æ¡ä»¶**: è¿è¡Œ `mini-agent` æˆ– `python -m mini_agent.cli`

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 179-197 | Description: CLI argument parsing
def parse_args() -> argparse.Namespace:
    """Parse command line arguments

    Returns:
        Parsed arguments
    """
    parser = argparse.ArgumentParser(
        description="Mini Agent - AI assistant with file tools and MCP support",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  mini-agent                              # Use current directory as workspace
  mini-agent --workspace /path/to/dir     # Use specific workspace directory
        """,
    )
    parser.add_argument(
        "--workspace",
        "-w",
        type=str,
        default=None,
        help="Workspace directory (default: current directory)",
    )
```

**æ•°æ®æµ**: CLI å‚æ•° â†’ workspace_dir

**å…³é”®ç‚¹**: æ”¯æŒ `--workspace` æŒ‡å®šå·¥ä½œç›®å½•ã€‚

#### æ­¥éª¤2: åŠ è½½é…ç½®
**è§¦å‘æ¡ä»¶**: è¿›å…¥ `run_agent`

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 333-352 | Description: Config loading flow
# 1. Load configuration from package directory
config_path = Config.get_default_config_path()

if not config_path.exists():
    print(f"{Colors.RED}âŒ Configuration file not found{Colors.RESET}")
    print()
    print(f"{Colors.BRIGHT_CYAN}ğŸ“¦ Configuration Search Path:{Colors.RESET}")
    print(f"  {Colors.DIM}1) mini_agent/config/config.yaml{Colors.RESET} (development)")
    print(f"  {Colors.DIM}2) ~/.mini-agent/config/config.yaml{Colors.RESET} (user)")
    print(f"  {Colors.DIM}3) <package>/config/config.yaml{Colors.RESET} (installed)")
    print()
    print(f"{Colors.BRIGHT_YELLOW}ğŸš€ Quick Setup (Recommended):{Colors.RESET}")
    print(f"  {Colors.BRIGHT_GREEN}curl -fsSL https://raw.githubusercontent.com/MiniMax-AI/Mini-Agent/main/scripts/setup-config.sh | bash{Colors.RESET}")
    print()
    print(f"{Colors.DIM}  This will automatically:{Colors.RESET}")
    print(f"{Colors.DIM}    â€¢ Create ~/.mini-agent/config/{Colors.RESET}")
```

**æ•°æ®æµ**: config.yaml â†’ Config å¯¹è±¡

**å…³é”®ç‚¹**: ç¼ºå¤±é…ç½®æ—¶ç›´æ¥è¿”å›å¹¶æç¤ºå¿«é€Ÿé…ç½®è„šæœ¬ã€‚

#### æ­¥éª¤3: åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
**è§¦å‘æ¡ä»¶**: é…ç½®åŠ è½½æˆåŠŸ

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 373-402 | Description: LLM client initialization
# 2. Initialize LLM client
from mini_agent.retry import RetryConfig as RetryConfigBase

# Convert configuration format
retry_config = RetryConfigBase(
    enabled=config.llm.retry.enabled,
    max_retries=config.llm.retry.max_retries,
    initial_delay=config.llm.retry.initial_delay,
    max_delay=config.llm.retry.max_delay,
    exponential_base=config.llm.retry.exponential_base,
    retryable_exceptions=(Exception,),
)

# Convert provider string to LLMProvider enum
provider = LLMProvider.ANTHROPIC if config.llm.provider.lower() == "anthropic" else LLMProvider.OPENAI

llm_client = LLMClient(
    api_key=config.llm.api_key,
    provider=provider,
    api_base=config.llm.api_base,
    model=config.llm.model,
    retry_config=retry_config if config.llm.retry.enabled else None,
)
```

**æ•°æ®æµ**: Config.llm â†’ LLMClient

**å…³é”®ç‚¹**: provider å†³å®š API è·¯å¾„åç¼€ã€‚

#### æ­¥éª¤4: åˆå§‹åŒ–å·¥å…·ä¸ç³»ç»Ÿæç¤ºè¯
**è§¦å‘æ¡ä»¶**: LLM åˆå§‹åŒ–å®Œæˆ

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 409-430 | Description: Tool init and system prompt
# 3. Initialize base tools (independent of workspace)
tools, skill_loader = await initialize_base_tools(config)

# 4. Add workspace-dependent tools
add_workspace_tools(tools, config, workspace_dir)

# 5. Load System Prompt (with priority search)
system_prompt_path = Config.find_config_file(config.agent.system_prompt_path)
if system_prompt_path and system_prompt_path.exists():
    system_prompt = system_prompt_path.read_text(encoding="utf-8")
    print(f"{Colors.GREEN}âœ… Loaded system prompt (from: {system_prompt_path}){Colors.RESET}")
else:
    system_prompt = "You are Mini-Agent, an intelligent assistant powered by MiniMax M2 that can help users complete various tasks."
    print(f"{Colors.YELLOW}âš ï¸  System prompt not found, using default{Colors.RESET}")

# 6. Inject Skills Metadata into System Prompt (Progressive Disclosure - Level 1)
if skill_loader:
    skills_metadata = skill_loader.get_skills_metadata_prompt()
    if skills_metadata:
        # Replace placeholder with actual metadata
        system_prompt = system_prompt.replace("{SKILLS_METADATA}", skills_metadata)
        print(f"{Colors.GREEN}âœ… Injected {len(skill_loader.loaded_skills)} skills metadata into system prompt{Colors.RESET}")
```

**æ•°æ®æµ**: Config.tools â†’ Tools åˆ—è¡¨ï¼›system_prompt.md â†’ system_prompt

**å…³é”®ç‚¹**: æŠ€èƒ½å…ƒä¿¡æ¯æŒ‰éœ€æ³¨å…¥ï¼Œé¿å…å…¨é‡åŠ è½½ã€‚

### å¼‚å¸¸å¤„ç†
- é…ç½®æ–‡ä»¶ç¼ºå¤±ï¼šç›´æ¥è¿”å›å¹¶æç¤ºè„šæœ¬ï¼ˆè§æ­¥éª¤2ï¼‰ã€‚
- é…ç½®è§£æå¤±è´¥ï¼šè¾“å‡ºé”™è¯¯å¹¶é€€å‡ºã€‚

### è®¾è®¡äº®ç‚¹
- å·¥å…·åˆ†ä¸º workspace æ— å…³ä¸ç›¸å…³ä¸¤ç±»ï¼Œåˆå§‹åŒ–é¡ºåºæ¸…æ™°ã€‚
- system prompt æ”¯æŒæŒ‰ä¼˜å…ˆçº§æœç´¢ï¼Œä¾¿äºæœ¬åœ°/ç”¨æˆ·/å®‰è£…åŒ…åˆ‡æ¢ã€‚

---

## æ ¸å¿ƒæµç¨‹ #2: Agent æ‰§è¡Œå¾ªç¯ä¸å·¥å…·è°ƒç”¨

### æ¦‚è¿°
Agent åœ¨æ¯æ­¥å¾ªç¯ä¸­è¿›è¡Œæ¶ˆæ¯æ‘˜è¦æ£€æŸ¥ã€è°ƒç”¨ LLM ç”Ÿæˆå“åº”ï¼Œå¹¶æ ¹æ® tool_calls æ‰§è¡Œå·¥å…·ä¸å›å†™ç»“æœã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    Agent->>Agent: summarize history
    Agent->>LLMClient: generate response
    LLMClient-->>Agent: content + tool_calls
    Agent->>Tools: execute tool_calls
    Tools-->>Agent: tool result
    Agent->>Agent: append messages
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: æ‘˜è¦æ£€æŸ¥ä¸ LLM è°ƒç”¨
**è§¦å‘æ¡ä»¶**: æ¯ä¸ª step å¼€å§‹

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 289-321 | Description: Summarize and LLM call
while step < self.max_steps:
    # Check and summarize message history to prevent context overflow
    await self._summarize_messages()

    # Get tool list for LLM call
    tool_list = list(self.tools.values())

    # Log LLM request and call LLM with Tool objects directly
    self.logger.log_request(messages=self.messages, tools=tool_list)

    try:
        response = await self.llm.generate(messages=self.messages, tools=tool_list)
    except Exception as e:
        # Check if it's a retry exhausted error
        from .retry import RetryExhaustedError

        if isinstance(e, RetryExhaustedError):
            error_msg = f"LLM call failed after {e.attempts} retries
Last error: {str(e.last_exception)}"
            print(f"
{Colors.BRIGHT_RED}âŒ Retry failed:{Colors.RESET} {error_msg}")
        else:
            error_msg = f"LLM call failed: {str(e)}"
            print(f"
{Colors.BRIGHT_RED}âŒ Error:{Colors.RESET} {error_msg}")
        return error_msg
```

**æ•°æ®æµ**: messages + tools â†’ LLMResponse

**å…³é”®ç‚¹**: LLM è°ƒç”¨å¤±è´¥ç›´æ¥è¿”å›é”™è¯¯å­—ç¬¦ä¸²å¹¶ç»“æŸå¾ªç¯ã€‚

#### æ­¥éª¤2: å·¥å…·æ‰§è¡Œ
**è§¦å‘æ¡ä»¶**: response.tool_calls éç©º

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 359-378 | Description: Tool call execution and argument formatting
for tool_call in response.tool_calls:
    tool_call_id = tool_call.id
    function_name = tool_call.function.name
    arguments = tool_call.function.arguments

    # Tool call header
    print(f"
{Colors.BRIGHT_YELLOW}ğŸ”§ Tool Call:{Colors.RESET} {Colors.BOLD}{Colors.CYAN}{function_name}{Colors.RESET}")

    # Arguments (formatted display)
    print(f"{Colors.DIM}   Arguments:{Colors.RESET}")
    # Truncate each argument value to avoid overly long output
    truncated_args = {}
    for key, value in arguments.items():
        value_str = str(value)
        if len(value_str) > 200:
            truncated_args[key] = value_str[:200] + "..."
        else:
            truncated_args[key] = value
    args_json = json.dumps(truncated_args, indent=2, ensure_ascii=False)
```

**æ•°æ®æµ**: tool_calls â†’ ToolResult â†’ Message(role="tool")

**å…³é”®ç‚¹**: ToolResult ä¼šè¢«è®°å½•åˆ°æ—¥å¿—å¹¶å†™å…¥æ¶ˆæ¯å†å²ã€‚

### å¼‚å¸¸å¤„ç†
- å·¥å…·ä¸å­˜åœ¨ï¼šè¿”å› `Unknown tool` é”™è¯¯ã€‚
- å·¥å…·æ‰§è¡Œå¼‚å¸¸ï¼šæ•è· traceback å¹¶åŒ…è£…ä¸º ToolResult.errorã€‚

### è®¾è®¡äº®ç‚¹
- å·¥å…·å‚æ•°æˆªæ–­é¿å…ç»ˆç«¯è¾“å‡ºè¿‡é•¿ã€‚
- ToolResult æˆåŠŸ/å¤±è´¥è·¯å¾„ç»Ÿä¸€è®°å½•ã€‚

---

## å…³é”®æœºåˆ¶è¡¥å……: å·¥å…·ç³»ç»Ÿ

### æ¦‚è¿°
å·¥å…·ç³»ç»Ÿå°† LLM çš„æ„å›¾è½åœ°ä¸ºå¯æ‰§è¡ŒåŠ¨ä½œï¼Œå¹¶é€šè¿‡ workspace è¾¹ç•Œã€è¡Œå·è¾“å‡ºå’Œ ToolResult çº¦æŸä¿è¯æ“ä½œå¯æ§ä¸å¯è¿½è¸ªã€‚

### æœºåˆ¶ #1: æ–‡ä»¶è¯»å–ï¼ˆè¡Œå· + æˆªæ–­ + workspace è§£æï¼‰
**è§¦å‘æ¡ä»¶**: LLM è°ƒç”¨ read_file

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/file_tools.py | Lines: 108-148 | Description: Read file with workspace resolution and line numbers
async def execute(self, path: str, offset: int | None = None, limit: int | None = None) -> ToolResult:
    """Execute read file."""
    try:
        file_path = Path(path)
        # Resolve relative paths relative to workspace_dir
        if not file_path.is_absolute():
            file_path = self.workspace_dir / file_path

        if not file_path.exists():
            return ToolResult(
                success=False,
                content="",
                error=f"File not found: {path}",
            )

        # Read file content with line numbers
        with open(file_path, encoding="utf-8") as f:
            lines = f.readlines()

        # Apply offset and limit
        start = (offset - 1) if offset else 0
        end = (start + limit) if limit else len(lines)
        if start < 0:
            start = 0
        if end > len(lines):
            end = len(lines)

        selected_lines = lines[start:end]

        # Format with line numbers (1-indexed)
        numbered_lines = []
        for i, line in enumerate(selected_lines, start=start + 1):
            # Remove trailing newline for formatting
            line_content = line.rstrip("\n")
            numbered_lines.append(f"{i:6d}|{line_content}")

        content = "\n".join(numbered_lines)

        # Apply token truncation if needed
        max_tokens = 32000
        content = truncate_text_by_tokens(content, max_tokens)
```

**æ•°æ®æµ**: path â†’ workspace resolve â†’ lines[] â†’ numbered content â†’ token truncation

**è®¾è®¡åŠ¨æœº**: è¡Œå·è¾“å‡ºæ”¯æŒç²¾ç¡®å¼•ç”¨ä¸åç»­ç¼–è¾‘ï¼›workspace ç»Ÿä¸€è§£æç›¸å¯¹è·¯å¾„ï¼Œé™ä½è¯¯è¯»ä¸è¶Šç•Œé£é™©ï¼›æˆªæ–­æ§åˆ¶è¾“å‡ºè§„æ¨¡ï¼Œé¿å…ä¸Šä¸‹æ–‡è†¨èƒ€ã€‚

### æœºåˆ¶ #2: æ–‡ä»¶å†™å…¥ä¸ç¼–è¾‘ï¼ˆç›®å½•åˆ›å»º + ç²¾ç¡®æ›¿æ¢ï¼‰
**è§¦å‘æ¡ä»¶**: LLM è°ƒç”¨ write_file / edit_file

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/file_tools.py | Lines: 195-207 | Description: Write file with workspace resolution and mkdir
async def execute(self, path: str, content: str) -> ToolResult:
    """Execute write file."""
    try:
        file_path = Path(path)
        # Resolve relative paths relative to workspace_dir
        if not file_path.is_absolute():
            file_path = self.workspace_dir / file_path

        # Create parent directories if they don't exist
        file_path.parent.mkdir(parents=True, exist_ok=True)

        file_path.write_text(content, encoding="utf-8")
        return ToolResult(success=True, content=f"Successfully wrote to {file_path}")
```

```python
# File: mini_agent/tools/file_tools.py | Lines: 256-281 | Description: Edit file with exact match replacement
async def execute(self, path: str, old_str: str, new_str: str) -> ToolResult:
    """Execute edit file."""
    try:
        file_path = Path(path)
        # Resolve relative paths relative to workspace_dir
        if not file_path.is_absolute():
            file_path = self.workspace_dir / file_path

        if not file_path.exists():
            return ToolResult(
                success=False,
                content="",
                error=f"File not found: {path}",
            )

        content = file_path.read_text(encoding="utf-8")

        if old_str not in content:
            return ToolResult(
                success=False,
                content="",
                error=f"Text not found in file: {old_str}",
            )

        new_content = content.replace(old_str, new_str)
        file_path.write_text(new_content, encoding="utf-8")
```

**æ•°æ®æµ**: path + content â†’ mkdir parents â†’ write; path + old_str â†’ exact match â†’ replace â†’ write

**è®¾è®¡åŠ¨æœº**: å†™å…¥æ—¶è‡ªåŠ¨åˆ›å»ºç›®å½•ï¼Œé™ä½å‰ç½®æ“ä½œå¤æ‚åº¦ï¼›ç¼–è¾‘è¦æ±‚ç²¾ç¡®åŒ¹é…ï¼Œé¿å…æ¨¡ç³Šæ›¿æ¢å¸¦æ¥çš„ä¸å¯æ§æ”¹åŠ¨ã€‚

### æœºåˆ¶ #3: Bash é•¿ä»»åŠ¡çš„åå°æ‰§è¡Œä¸å›æ”¶
**è§¦å‘æ¡ä»¶**: LLM è°ƒç”¨ bash(run_in_background=true) / bash_output / bash_kill

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/bash_tool.py | Lines: 302-370 | Description: Background bash execution with bash_id
async def execute(
    self,
    command: str,
    timeout: int = 120,
    run_in_background: bool = False,
) -> ToolResult:
    """Execute shell command with optional background execution.

    Args:
        command: The shell command to execute
        timeout: Timeout in seconds (default: 120, max: 600)
        run_in_background: Set true to run command in background

    Returns:
        BashExecutionResult with command output and status
    """

    try:
        # Validate timeout
        if timeout > 600:
            timeout = 600
        elif timeout < 1:
            timeout = 120

        # Prepare shell-specific command execution
        if self.is_windows:
            # Windows: Use PowerShell with appropriate encoding
            shell_cmd = ["powershell.exe", "-NoProfile", "-Command", command]
        else:
            # Unix/Linux/macOS: Use bash
            shell_cmd = command

        if run_in_background:
            # Background execution: Create isolated process
            bash_id = str(uuid.uuid4())[:8]

            # Start background process with combined stdout/stderr
            if self.is_windows:
                process = await asyncio.create_subprocess_exec(
                    *shell_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.STDOUT,
                )
            else:
                process = await asyncio.create_subprocess_shell(
                    shell_cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.STDOUT,
                )

            # Create background shell and add to manager
            bg_shell = BackgroundShell(bash_id=bash_id, command=command, process=process, start_time=time.time())
            BackgroundShellManager.add(bg_shell)

            # Start monitoring task
            await BackgroundShellManager.start_monitor(bash_id)

            # Return immediately with bash_id
            message = f"Command started in background. Use bash_output to monitor (bash_id='{bash_id}')."
            formatted_content = f"{message}

Command: {command}
Bash ID: {bash_id}"

            return BashOutputResult(
                success=True,
                content=formatted_content,
                stdout=f"Background command started with ID: {bash_id}",
                stderr="",
                exit_code=0,
                bash_id=bash_id,
            )
```

```python
# File: mini_agent/tools/bash_tool.py | Lines: 489-512 | Description: Read incremental output by bash_id
try:
    # Get background shell from manager
    bg_shell = BackgroundShellManager.get(bash_id)
    if not bg_shell:
        available_ids = BackgroundShellManager.get_available_ids()
        return BashOutputResult(
            success=False,
            error=f"Shell not found: {bash_id}. Available: {available_ids or 'none'}",
            stdout="",
            stderr="",
            exit_code=-1,
        )

    # Get new output
    new_lines = bg_shell.get_new_output(filter_pattern=filter_str)
    stdout = "
".join(new_lines) if new_lines else ""

    return BashOutputResult(
        success=True,
        stdout=stdout,
        stderr="",  # Background shells combine stdout/stderr
        exit_code=bg_shell.exit_code if bg_shell.exit_code is not None else 0,
        bash_id=bash_id,
    )
```

```python
# File: mini_agent/tools/bash_tool.py | Lines: 567-586 | Description: Terminate background bash by bash_id
try:
    # Get remaining output before termination
    bg_shell = BackgroundShellManager.get(bash_id)
    if bg_shell:
        remaining_lines = bg_shell.get_new_output()
    else:
        remaining_lines = []

    # Terminate through manager (handles all cleanup)
    bg_shell = await BackgroundShellManager.terminate(bash_id)

    # Get remaining output
    stdout = "
".join(remaining_lines) if remaining_lines else ""

    return BashOutputResult(
        success=True,
        stdout=stdout,
        stderr="",
        exit_code=bg_shell.exit_code if bg_shell.exit_code is not None else 0,
        bash_id=bash_id,
    )
```

**æ•°æ®æµ**: command â†’ bash_id â†’ BackgroundShell â†’ bash_output/kill â†’ ToolResult

**è®¾è®¡åŠ¨æœº**: åå°æ‰§è¡Œé¿å…é˜»å¡ä¸»å¯¹è¯ï¼›bash_output è¿”å›å¢é‡è¾“å‡ºï¼Œä¾¿äºç›‘æ§é•¿ä»»åŠ¡ï¼›bash_kill ç»Ÿä¸€å›æ”¶èµ„æºï¼Œé¿å…åƒµå°¸è¿›ç¨‹ã€‚

### è®¾è®¡äº®ç‚¹
- **ä¸€è‡´æ€§**: å·¥å…·ç»Ÿä¸€ä»¥ ToolResult è¿”å›ç»“æœï¼Œä¾¿äºåœ¨æ¶ˆæ¯å¾ªç¯ä¸­å¤„ç†æˆåŠŸ/å¤±è´¥è·¯å¾„ã€‚
- **å¯æ§æ€§**: workspace ä½œä¸ºè¾¹ç•Œã€bash_id ä½œä¸ºå…³è”é”®ï¼Œå¢å¼ºå·¥å…·è°ƒç”¨çš„å¯è¿½è¸ªæ€§ã€‚
- **å¯ç»´æŠ¤æ€§**: å·¥å…·èŒè´£æ¸…æ™°ã€çº¦æŸæ˜¾å¼ï¼Œä¾¿äºæ‰©å±•ä¸æ›¿æ¢å®ç°ã€‚

æ›´å¤šå·¥å…·æ¸…å•ä¸è¡Œä¸ºå¤‡ä»½è§ `12-tools-design.md`ã€‚

---

## æ ¸å¿ƒæµç¨‹ #3: æ¶ˆæ¯æ‘˜è¦æœºåˆ¶

### æ¦‚è¿°
å½“æ¶ˆæ¯ token æ•°è¶…è¿‡é™åˆ¶æ—¶ï¼ŒAgent è‡ªåŠ¨å°†ç”¨æˆ·ä¹‹é—´çš„æ‰§è¡Œè¿‡ç¨‹è¿›è¡Œæ‘˜è¦ï¼Œä¿ç•™ç”¨æˆ·æ„å›¾å¹¶å‹ç¼©ä¸Šä¸‹æ–‡ã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    Agent->>Agent: estimate tokens
    Agent->>Agent: summarize execution rounds
    Agent->>LLMClient: generate summary
    LLMClient-->>Agent: summary text
    Agent->>Agent: replace message history
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: Token é˜ˆå€¼ä¸ workspace æ³¨å…¥
**è§¦å‘æ¡ä»¶**: Agent åˆå§‹åŒ–

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 45-67 | Description: Token limit and workspace injection
def __init__(
    self,
    llm_client: LLMClient,
    system_prompt: str,
    tools: list[Tool],
    max_steps: int = 50,
    workspace_dir: str = "./workspace",
    token_limit: int = 80000,  # Summary triggered when tokens exceed this value
):
    self.llm = llm_client
    self.tools = {tool.name: tool for tool in tools}
    self.max_steps = max_steps
    self.token_limit = token_limit
    self.workspace_dir = Path(workspace_dir)

    # Ensure workspace exists
    self.workspace_dir.mkdir(parents=True, exist_ok=True)

    # Inject workspace information into system prompt if not already present
    if "Current Workspace" not in system_prompt:
        workspace_info = f"

## Current Workspace
You are currently working in: `{self.workspace_dir.absolute()}`
All relative paths will be resolved relative to this directory."
        system_prompt = system_prompt + workspace_info
```

#### æ­¥éª¤2: è§¦å‘æ‘˜è¦ä¸æ¶ˆæ¯é‡æ„
**è§¦å‘æ¡ä»¶**: token è¶…è¿‡é˜ˆå€¼

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 142-160 | Description: Summarization trigger check
async def _summarize_messages(self):
    """Message history summarization: summarize conversations between user messages when tokens exceed limit

    Strategy (Agent mode):
    - Keep all user messages (these are user intents)
    - Summarize content between each user-user pair (agent execution process)
    - If last round is still executing (has agent/tool messages but no next user), also summarize
    - Structure: system -> user1 -> summary1 -> user2 -> summary2 -> user3 -> summary3 (if executing)
    """
    # Skip check if we just completed a summary (wait for next LLM call to update api_total_tokens)
    if self._skip_next_token_check:
        self._skip_next_token_check = False
        return

    estimated_tokens = self._estimate_tokens()
```

#### æ­¥éª¤3: ç”Ÿæˆæ‘˜è¦æ–‡æœ¬
**è§¦å‘æ¡ä»¶**: éœ€è¦ä¸ºæ‰§è¡Œè¿‡ç¨‹ç”Ÿæˆæ‘˜è¦

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/agent.py | Lines: 261-270 | Description: Summary generation call
summary_msg = Message(role="user", content=summary_prompt)
response = await self.llm.generate(
    messages=[
        Message(
            role="system",
            content="You are an assistant skilled at summarizing Agent execution processes.",
        ),
        summary_msg,
    ]
)
```

### å¼‚å¸¸å¤„ç†
- æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼šå›é€€ä¸ºç®€å•æ–‡æœ¬æ‘˜è¦ï¼ˆè§ `_create_summary` å¼‚å¸¸å¤„ç†åˆ†æ”¯ï¼‰ã€‚

### è®¾è®¡äº®ç‚¹
- æ‘˜è¦åªæ›¿æ¢æ‰§è¡Œè¿‡ç¨‹ï¼Œä¸ä¸¢å¤±ç”¨æˆ·è¾“å…¥ã€‚
- token ä¼°ç®—ä¸ API usage åŒé‡è§¦å‘ã€‚

---

## æ ¸å¿ƒæµç¨‹ #4: Skills ä¸ MCP åŠ è½½

### æ¦‚è¿°
Skills ä¸ MCP å·¥å…·åœ¨å¯åŠ¨æ—¶è¢«åŠ è½½ä¸ºå¯ç”¨å·¥å…·é›†åˆï¼ŒSkills é‡‡ç”¨ Progressive Disclosureï¼ŒMCP é€šè¿‡ stdio å¯åŠ¨å¤–éƒ¨æœåŠ¡å™¨ã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    CLI->>SkillLoader: discover SKILL.md
    CLI->>Agent: inject skills metadata
    CLI->>MCP: load mcp.json
    MCP->>Server: start stdio servers
    MCP-->>CLI: tool schemas
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: Skills å‘ç°ä¸å·¥å…·æ³¨å…¥
**è§¦å‘æ¡ä»¶**: initialize_base_tools

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/skill_tool.py | Lines: 57-83 | Description: Skill tool creation
def create_skill_tools(
    skills_dir: str = "./skills",
) -> tuple[List[Tool], Optional[SkillLoader]]:
    """
    Create skill tool for Progressive Disclosure

    Only provides get_skill tool - the agent uses metadata in system prompt
    to know what skills are available, then loads them on-demand.
    """
    # Create skill loader
    loader = SkillLoader(skills_dir)

    # Discover and load skills
    skills = loader.discover_skills()
    print(f"âœ… Discovered {len(skills)} Claude Skills")

    # Create only the get_skill tool (Progressive Disclosure Level 2)
    tools = [
        GetSkillTool(loader),
    ]

    return tools, loader
```

#### æ­¥éª¤2: Skills å…ƒä¿¡æ¯æ³¨å…¥
**è§¦å‘æ¡ä»¶**: skill_loader ä¸ä¸ºç©º

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/cli.py | Lines: 424-430 | Description: Skills metadata injection
if skill_loader:
    skills_metadata = skill_loader.get_skills_metadata_prompt()
    if skills_metadata:
        # Replace placeholder with actual metadata
        system_prompt = system_prompt.replace("{SKILLS_METADATA}", skills_metadata)
        print(f"{Colors.GREEN}âœ… Injected {len(skill_loader.loaded_skills)} skills metadata into system prompt{Colors.RESET}")
```

#### æ­¥éª¤3: MCP å·¥å…·åŠ è½½
**è§¦å‘æ¡ä»¶**: enable_mcp ä¸º true

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/mcp_loader.py | Lines: 154-170 | Description: MCP tools loading entry
async def load_mcp_tools_async(config_path: str = "mcp.json") -> list[Tool]:
    """
    Load MCP tools from config file.

    This function:
    1. Reads the MCP config file
    2. Starts MCP server processes
    3. Connects to each server
    4. Fetches tool definitions
    5. Wraps them as Tool objects
    """
    global _mcp_connections

    config_file = Path(config_path)

    if not config_file.exists():
        print(f"MCP config not found: {config_path}")
        return []
```

```python
# File: mini_agent/tools/mcp_loader.py | Lines: 192-210 | Description: MCP server connection loop
for server_name, server_config in mcp_servers.items():
    if server_config.get("disabled", False):
        print(f"Skipping disabled server: {server_name}")
        continue

    command = server_config.get("command")
    args = server_config.get("args", [])
    env = server_config.get("env", {})

    if not command:
        print(f"No command specified for server: {server_name}")
        continue

    connection = MCPServerConnection(server_name, command, args, env)
    success = await connection.connect()

    if success:
        _mcp_connections.append(connection)
        all_tools.extend(connection.tools)
```

### å¼‚å¸¸å¤„ç†
- Skills åŠ è½½å¤±è´¥ï¼šæ‰“å°è­¦å‘Šå¹¶ç»§ç»­å¯åŠ¨ã€‚
- MCP è¿æ¥å¤±è´¥ï¼šæ•è·å¼‚å¸¸å¹¶ç»§ç»­åŠ è½½å…¶ä»– serverã€‚

### è®¾è®¡äº®ç‚¹
- Skills ä¸ MCP å‡å¯åœ¨é…ç½®ä¸­å¼€å…³ï¼Œæ–¹ä¾¿è£å‰ªèƒ½åŠ›ã€‚
- MCP ä½¿ç”¨ AsyncExitStack ä¿è¯è¿æ¥ç”Ÿå‘½å‘¨æœŸå¯æ§ã€‚

---

## æ ¸å¿ƒæµç¨‹ #5: Session Note è®°å¿†æŒä¹…åŒ–

### æ¦‚è¿°
SessionNoteTool å°†è®°å¿†ä»¥ JSON å½¢å¼å†™å…¥ workspaceï¼ŒRecallNoteTool è´Ÿè´£è¯»å–ä¸æ ¼å¼åŒ–å±•ç¤ºã€‚

### æ—¶åºå›¾
```mermaid
sequenceDiagram
    Agent->>SessionNoteTool: record_note(content, category)
    SessionNoteTool->>MemoryFile: load existing notes
    SessionNoteTool->>MemoryFile: write updated notes
    Agent->>RecallNoteTool: recall_notes(category?)
    RecallNoteTool->>MemoryFile: read notes
    RecallNoteTool-->>Agent: formatted notes
```

### è¯¦ç»†æ­¥éª¤

#### æ­¥éª¤1: è®°å½•è®°å¿†
**è§¦å‘æ¡ä»¶**: LLM è°ƒç”¨ record_note

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/note_tool.py | Lines: 91-119 | Description: Record session note
async def execute(self, content: str, category: str = "general") -> ToolResult:
    """Record a session note.

    Args:
        content: The information to record
        category: Category/tag for this note

    Returns:
        ToolResult with success status
    """
    try:
        # Load existing notes
        notes = self._load_from_file()

        # Add new note with timestamp
        note = {
            "timestamp": datetime.now().isoformat(),
            "category": category,
            "content": content,
        }
        notes.append(note)

        # Save back to file
        self._save_to_file(notes)

        return ToolResult(
            success=True,
            content=f"Recorded note: {content} (category: {category})",
        )
```

**æ•°æ®æµ**: content/category â†’ notes[] â†’ .agent_memory.json

**å…³é”®ç‚¹**: ä»…åœ¨éœ€è¦è®°å½•æ—¶å†™å…¥æ–‡ä»¶ï¼Œé¿å…æ— è°“ IOã€‚

#### æ­¥éª¤2: å¬å›è®°å¿†
**è§¦å‘æ¡ä»¶**: LLM è°ƒç”¨ recall_notes

**æ ¸å¿ƒä»£ç **:
```python
# File: mini_agent/tools/note_tool.py | Lines: 163-206 | Description: Recall session notes
async def execute(self, category: str = None) -> ToolResult:
    """Recall session notes.

    Args:
        category: Optional category filter

    Returns:
        ToolResult with notes content
    """
    try:
        if not self.memory_file.exists():
            return ToolResult(
                success=True,
                content="No notes recorded yet.",
            )

        notes = json.loads(self.memory_file.read_text())

        if not notes:
            return ToolResult(
                success=True,
                content="No notes recorded yet.",
            )

        # Filter by category if specified
        if category:
            notes = [n for n in notes if n.get("category") == category]
            if not notes:
                return ToolResult(
                    success=True,
                    content=f"No notes found in category: {category}",
                )

        # Format notes for display
        formatted = []
        for idx, note in enumerate(notes, 1):
            timestamp = note.get("timestamp", "unknown time")
            cat = note.get("category", "general")
            content = note.get("content", "")
            formatted.append(f"{idx}. [{cat}] {content}\n   (recorded at {timestamp})")

        result = "Recorded Notes:\n" + "\n".join(formatted)

        return ToolResult(success=True, content=result)
```

**æ•°æ®æµ**: .agent_memory.json â†’ filter(category) â†’ formatted string

### å¼‚å¸¸å¤„ç†
- æ–‡ä»¶ä¸å­˜åœ¨æˆ–è§£æå¤±è´¥æ—¶è¿”å›ç©ºç»“æœæç¤ºï¼Œä¸ä¸­æ–­ä¸»æµç¨‹ã€‚

### è®¾è®¡äº®ç‚¹
- lazy loading ä¸å»¶è¿Ÿåˆ›å»ºæ–‡ä»¶ï¼Œé¿å…æ— ç”¨æ–‡ä»¶å ç”¨ã€‚
- é€šè¿‡ category æ”¯æŒç®€å•åˆ†ç±»æ£€ç´¢ã€‚
